{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49a3d29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic similarity score V2: {'score': 1, 'key': 'similarity_v2'}\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------\n",
    "# Execute LLM-as-Judge V2 Evaluator\n",
    "# -----------------------------------------------------\n",
    "sample_run = {\n",
    "  \"name\": \"Sample Run\",\n",
    "  \"inputs\": {\n",
    "    \"question\": \"Is LangSmith natively integrated with LangChain?\"\n",
    "  },\n",
    "  \"outputs\": {\n",
    "    \"output\": \"No, LangSmith is NOT integrated with LangChain.\"\n",
    "  },\n",
    "  \"is_root\": True,\n",
    "  \"status\": \"success\",\n",
    "  \"extra\": {\n",
    "    \"metadata\": {\n",
    "      \"key\": \"value\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "sample_example = {\n",
    "  \"inputs\": {\n",
    "    \"question\": \"Is LangSmith natively integrated with LangChain?\"\n",
    "  },\n",
    "  \"outputs\": {\n",
    "    \"output\": \"Yes, LangSmith is natively integrated with LangChain, as well as LangGraph.\"\n",
    "  },\n",
    "  \"metadata\": {\n",
    "    \"dataset_split\": [\n",
    "      \"AI generated\",\n",
    "      \"base\"\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\n",
    "similarity_score = compare_semantic_similarity_v2(sample_run, sample_example)\n",
    "print(f\"Semantic similarity score V2: {similarity_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92b39083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM-as-Judge V2 evaluator defined.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------\n",
    "# LLM-as-Judge V2 (Using Run and Example Schemas)\n",
    "# -----------------------------------------------------\n",
    "def compare_semantic_similarity_v2(root_run: dict, example: dict):\n",
    "    # Extract data using dictionary access (emulating Pydantic object access for simplicity)\n",
    "    input_question = example[\"inputs\"][\"question\"]\n",
    "    reference_response = example[\"outputs\"][\"output\"]\n",
    "    run_response = root_run[\"outputs\"][\"output\"]\n",
    "\n",
    "    # Use Groq LLM with LangChain's structured output parser\n",
    "    structured_llm = llm_client.with_structured_output(Similarity_Score)\n",
    "    \n",
    "    # Define the system prompt for the evaluator\n",
    "    system_prompt = (\n",
    "        \"You are a semantic similarity evaluator. Compare the meanings of two responses to a question, \"\n",
    "        \"Reference Response and New Response, where the reference is the correct answer, and we are trying to judge if the new response is similar. \"\n",
    "        \"Provide a score between 1 and 10, where 1 means completely unrelated, and 10 means identical in meaning. \"\n",
    "        \"Your only output must be the JSON object matching the requested schema.\"\n",
    "    )\n",
    "\n",
    "    # Invoke the structured LLM chain\n",
    "    completion = structured_llm.invoke([\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Question: {input_question}\\n Reference Response: {reference_response}\\n Run Response: {run_response}\"}\n",
    "    ])\n",
    "\n",
    "    # Return the structured score\n",
    "    return {\"score\": completion.similarity_score, \"key\": \"similarity_v2\"}\n",
    "\n",
    "print(\"LLM-as-Judge V2 evaluator defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "605df3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic similarity score: {'score': 1, 'key': 'similarity'}\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------\n",
    "# Execute LLM-as-Judge Evaluator\n",
    "# -----------------------------------------------------\n",
    "# From Dataset Example\n",
    "inputs = {\n",
    "  \"question\": \"Is LangSmith natively integrated with LangChain?\"\n",
    "}\n",
    "reference_outputs = {\n",
    "  \"output\": \"Yes, LangSmith is natively integrated with LangChain, as well as LangGraph.\"\n",
    "}\n",
    "\n",
    "# From Run (Intentionally Wrong)\n",
    "outputs = {\n",
    "  \"output\": \"No, LangSmith is NOT integrated with LangChain.\"\n",
    "}\n",
    "\n",
    "# Run the LLM-as-Judge evaluation\n",
    "similarity_score = compare_semantic_similarity(inputs, reference_outputs, outputs)\n",
    "print(f\"Semantic similarity score: {similarity_score}\")\n",
    "\n",
    "# NOTE: We expect to see a low score due to the conflicting answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e8ea391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM-as-Judge evaluator 'compare_semantic_similarity' (Groq/Structured) defined.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------\n",
    "# Pydantic Schema for Structured Output\n",
    "# -----------------------------------------------------\n",
    "class Similarity_Score(BaseModel):\n",
    "    similarity_score: int = Field(description=\"Semantic similarity score between 1 and 10, where 1 means unrelated and 10 means identical.\")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# LLM-as-Judge Evaluator (TWEAK: Groq with Structured Output)\n",
    "# -----------------------------------------------------\n",
    "def compare_semantic_similarity(inputs: dict, reference_outputs: dict, outputs: dict):\n",
    "    input_question = inputs[\"question\"]\n",
    "    reference_response = reference_outputs[\"output\"]\n",
    "    run_response = outputs[\"output\"]\n",
    "\n",
    "    # TWEAK: Use Groq LLM with LangChain's structured output parser\n",
    "    structured_llm = llm_client.with_structured_output(Similarity_Score)\n",
    "    \n",
    "    # Define the system prompt for the evaluator\n",
    "    system_prompt = (\n",
    "        \"You are a semantic similarity evaluator. Compare the meanings of two responses to a question, \"\n",
    "        \"Reference Response and New Response, where the reference is the correct answer, and we are trying to judge if the new response is similar. \"\n",
    "        \"Provide a score between 1 and 10, where 1 means completely unrelated, and 10 means identical in meaning. \"\n",
    "        \"Your only output must be the JSON object matching the requested schema.\"\n",
    "    )\n",
    "\n",
    "    # Invoke the structured LLM chain\n",
    "    completion = structured_llm.invoke([\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Question: {input_question}\\n Reference Response: {reference_response}\\n Run Response: {run_response}\"}\n",
    "    ])\n",
    "\n",
    "    # The completion is now a Pydantic object, matching the original lesson's goal\n",
    "    return {\"score\": completion.similarity_score, \"key\": \"similarity\"}\n",
    "\n",
    "print(\"LLM-as-Judge evaluator 'compare_semantic_similarity' (Groq/Structured) defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aeb5c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete. Langsmith and Groq clients initialized.\n",
      "Simple 'correct_label' evaluator defined.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------\n",
    "# CELL 1: Setup & Simple Custom Evaluator\n",
    "# -----------------------------------------------------\n",
    "import os\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "from langsmith import Client\n",
    "from langchain_groq import ChatGroq \n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Suppress the specific LangChain Deprecation Warning (Not Tqdm, but good practice)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"langchain\")\n",
    "\n",
    "# Load environment variables (LANGCHAIN_API_KEY, GROQ_API_KEY, etc.)\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Initialize Groq Client and Langsmith Client\n",
    "# Set low temp for evaluation consistency\n",
    "MODEL_NAME = \"llama-3.3-70b-versatile\"\n",
    "llm_client = ChatGroq(model=MODEL_NAME, temperature=0.0) \n",
    "client = Client()\n",
    "\n",
    "print(\"Setup Complete. Langsmith and Groq clients initialized.\")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Simple Custom Evaluator\n",
    "# -----------------------------------------------------\n",
    "def correct_label(inputs: dict, reference_outputs: dict, outputs: dict) -> dict:\n",
    "  \"\"\"A very simple evaluator comparing model output to a reference 'output'.\"\"\"\n",
    "  # NOTE: We are comparing the 'output' keys from the provided dicts\n",
    "  score = outputs.get(\"output\") == reference_outputs.get(\"output\") \n",
    "  return {\"score\": int(score), \"key\": \"correct_label\"}\n",
    "\n",
    "print(\"Simple 'correct_label' evaluator defined.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langsmith_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
